{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from imblearn.over_sampling import  SMOTE \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils_veolia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_raw, output_raw, input_submission = utils_veolia.load_data()\n",
    "input_preprocessed = utils_veolia.preprocess(input_raw, year=2014, more_features=False)\n",
    "input_train, output_train, input_test, output_test = utils_veolia.split_train_test_Kfold(output_raw, input_preprocessed, year=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_train_duplicate, output_train_duplicate = utils_veolia.data_augmentation_basic(input_train, output_train, repetitions = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=10, max_depth=20)\n",
    "GBC = GradientBoostingClassifier(n_estimators=10, max_depth=20)\n",
    "ABC = AdaBoostClassifier(n_estimators=5, algorithm='SAMME.R')\n",
    "classifier = ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SAMME.R'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators_list = [3,5,10,20]\n",
    "algos = ['SAMME.R', 'SAMME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855670120841\n",
      "0.000462807022448\n"
     ]
    }
   ],
   "source": [
    "predict_proba = True\n",
    "smote = True\n",
    "test_size = 0.5\n",
    "scores_list = []\n",
    "n_estimators_list = [10]\n",
    "algos = ['SAMME']\n",
    "list(itertools.product(n_estimators_list, algos))\n",
    "\n",
    "results_cross_val = {}\n",
    "    \n",
    "for tupl in list(itertools.product(n_estimators_list, algos)):\n",
    "    classifier = AdaBoostClassifier(n_estimators=tupl[0], algorithm=tupl[1])\n",
    "    for i in range(20):\n",
    "        YEAR = 2014\n",
    "        input_preprocessed = utils_veolia.preprocess(input_raw, year=YEAR, more_features=True)\n",
    "        input_train, output_train, input_test, output_test = utils_veolia.split_train_test_stratified_shuffle(output_raw, input_preprocessed, test_size,YEAR)\n",
    "\n",
    "        if smote:\n",
    "            # SMOTE over sampling\n",
    "            sm = SMOTE(random_state=42)\n",
    "            input_train_duplicate, output_train_duplicate = sm.fit_sample(input_train, output_train[str(YEAR)])\n",
    "        else:\n",
    "            # simple duplication\n",
    "            input_train_duplicate, output_train_duplicate = utils_veolia.data_augmentation_basic(input_train, output_train, year = YEAR,repetitions = 8)\n",
    "\n",
    "        X_train = input_train_duplicate\n",
    "        #utils_veolia.print_repartition(output_train_duplicate)\n",
    "        Y_train = output_train_duplicate\n",
    "\n",
    "        X_test = input_test\n",
    "        Y_test_2014 = utils_veolia.preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "        rdm_1 = classifier\n",
    "        rdm_1.fit(X_train, Y_train)\n",
    "\n",
    "        if predict_proba:\n",
    "            y_pred_2014 = rdm_1.predict_proba(X_test)\n",
    "            y_pred_2014 = y_pred_2014[:,1]\n",
    "        else:\n",
    "            y_pred_2014 = rdm_1.predict(X_test)\n",
    "            print(classification_report(Y_test_2014,y_pred_2014))\n",
    "\n",
    "        YEAR = 2015\n",
    "        input_preprocessed = utils_veolia.preprocess(input_raw, year=YEAR, more_features=True)\n",
    "        input_train, output_train, input_test, output_test = utils_veolia.split_train_test_stratified_shuffle(output_raw, input_preprocessed, test_size=test_size, year=YEAR)\n",
    "\n",
    "        if smote:\n",
    "            # SMOTE over sampling\n",
    "            sm = SMOTE(random_state=42)\n",
    "            input_train_duplicate, output_train_duplicate = sm.fit_sample(input_train, output_train[str(YEAR)])\n",
    "        else:\n",
    "            # simple duplication\n",
    "            input_train_duplicate, output_train_duplicate = utils_veolia.data_augmentation_basic(input_train, output_train, year = YEAR,repetitions = 8)\n",
    "\n",
    "\n",
    "        X_train = input_train_duplicate\n",
    "        #utils_veolia.print_repartition(output_train_duplicate)\n",
    "        Y_train = output_train_duplicate\n",
    "\n",
    "        X_test = input_test\n",
    "        Y_test_2015 = utils_veolia.preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "        rdm_2 = classifier\n",
    "        rdm_2.fit(X_train, Y_train)\n",
    "\n",
    "        if predict_proba:\n",
    "            y_pred_2015 = rdm_1.predict_proba(X_test)\n",
    "            y_pred_2015 = y_pred_2015[:,1]\n",
    "        else:\n",
    "            y_pred_2015 = rdm_1.predict(X_test)\n",
    "            print(classification_report(Y_test_2015,y_pred_2015))\n",
    "\n",
    "        pred = np.array([y_pred_2014,y_pred_2015]).T\n",
    "        true = np.array([Y_test_2014,Y_test_2015]).T\n",
    "\n",
    "        scores_list += [utils_veolia.score_function(true, pred)]\n",
    "\n",
    "    #print scores_list\n",
    "    print np.mean(scores_list)\n",
    "    print np.var(scores_list)\n",
    "    results_cross_val[tupl] = (np.mean(scores_list), np.var(scores_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions for the submission data\n",
    "input_submission_2014 = utils_veolia.preprocess(input_submission, year=2014, more_features=True)\n",
    "sub_1 = rdm_1.predict_proba(input_submission_2014)\n",
    "input_submission_2015 = utils_veolia.preprocess(input_submission, year=2015, more_features=True)\n",
    "sub_2 = rdm_2.predict_proba(input_submission_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submission formating\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = input_submission.index.tolist()\n",
    "submission['2014'] = sub_1[:,1]\n",
    "submission['2015'] = sub_2[:,1]\n",
    "submission = submission.set_index('Id')\n",
    "submission.to_csv('../submissions/data_augmentation_ABC_new.csv',index=True,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/winning_ABC_2015.pkl']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rdm_1, '../model/winning_ABC_2014.pkl')\n",
    "joblib.dump(rdm_2, '../model/winning_ABC_2015.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
