{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "\n",
    "INPUT_TRAIN = DATA_PATH+'input_train.csv'\n",
    "OUTPUT_TRAIN = DATA_PATH+'output_train.csv'\n",
    "INPUT_SUBMISSION = DATA_PATH+'input_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_train = pd.read_csv(INPUT_TRAIN,index_col='Id')\n",
    "output_train = pd.read_csv(OUTPUT_TRAIN,sep=';',index_col='Id')\n",
    "input_submission = pd.read_csv(INPUT_SUBMISSION ,index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19427, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the rows with a canalisation breaks\n",
    "ID_2014 = output_train[output_train['2014']==1].index.tolist()\n",
    "ID_2015 = output_train[output_train['2015']==1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of breaks in 2014: (53, 7)\n",
      "Dimension of breaks in 2015: (37, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of breaks in 2014: {0}\".format(input_train.iloc[ID_2014].shape))\n",
    "print(\"Dimension of breaks in 2015: {0}\".format(input_train.iloc[ID_2015].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to preprocess before splitting into test and train data because get_dummies will only take into account existing categories, thus there are less columns in the test set if we preprocess after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(dataframe,year=2014):\n",
    "    X = dataframe\n",
    "    \n",
    "    # The relevant value is the age of the pipes\n",
    "    X['Age'] = year - X['YearConstruction']\n",
    "    X = X.fillna(10000)\n",
    "    \n",
    "    # How long has it been since last failure\n",
    "    X['YearsOldLastFailure'] = year - X['YearLastFailureObserved']\n",
    "\n",
    "    # Categorical data\n",
    "    X = pd.concat([X,pd.get_dummies(X['Feature1'])],axis=1)\n",
    "    X = pd.concat([X,pd.get_dummies(X['Feature2'])],axis=1)\n",
    "    X = pd.concat([X,pd.get_dummies(X['Feature4'])],axis=1)\n",
    "    \n",
    "    X = X.drop([\"YearConstruction\",\"YearLastFailureObserved\",\"Feature1\",\"Feature2\",\"Feature4\"],axis=1)\n",
    "   \n",
    "    X['Feature3'] = normalize(X['Feature3']).tolist()[0]\n",
    "    X['Length'] = normalize(X['Length']).tolist()[0]\n",
    "    X['Age'] = normalize(X['Age']).tolist()[0]\n",
    "    X['YearsOldLastFailure'] = normalize(X['YearsOldLastFailure']).tolist()[0]\n",
    "     \n",
    "    col = X.columns[4:]\n",
    "    for c in col:\n",
    "        for u in col:\n",
    "            X[c+u] = X[c]*X[u]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_train = preprocess(input_train,year = 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids_2014 = [ID_2014[w] for w in np.random.randint(0,high=len(ID_2014),size=int(perc*len(ID_2014)))] + np.random.randint(0,high=input_train.shape[0],size=int(perc*len(ID_2014))).tolist()\n",
    "test_ids_2015 = [ID_2015[w] for w in np.random.randint(0,high=len(ID_2015),size=int(perc*len(ID_2015)))] + np.random.randint(0,high=input_train.shape[0],size=int(perc*len(ID_2015))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 94) (88, 2)\n"
     ]
    }
   ],
   "source": [
    "input_test = pd.concat([input_train.loc[test_ids_2014],input_train.loc[test_ids_2015]])\n",
    "output_test = pd.concat([output_train.loc[test_ids_2014],output_train.loc[test_ids_2015]])\n",
    "\n",
    "print(input_test.shape,output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID_2014_train = [w for w in ID_2014 if w not in test_ids_2014]\n",
    "ID_2015_train = [w for w in ID_2015 if w not in test_ids_2015]\n",
    "ID_train = [w for w in output_train.index if w not in (test_ids_2014 + test_ids_2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augment data with breaks to counter unbalanced dataset only for training\n",
    "REPETITIONS = 7\n",
    "for k in range(0,REPETITIONS):\n",
    "    input_train = pd.concat([input_train.loc[ID_2014_train],input_train.loc[ID_2015_train],input_train.loc[ID_train]])\n",
    "    output_train = pd.concat([output_train.loc[ID_2014_train],output_train.loc[ID_2015_train],output_train.loc[ID_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32769, 94) (32769, 2)\n",
      "(88, 94) (88, 2)\n"
     ]
    }
   ],
   "source": [
    "print(input_train.shape, output_train.shape)\n",
    "print(input_test.shape, output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_output(dataframe,year=2014):\n",
    "    '''\n",
    "    Selects the right colum for the year studied\n",
    "    '''\n",
    "    return dataframe[str(year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.66      0.77        62\n",
      "          1       0.51      0.85      0.64        26\n",
      "\n",
      "avg / total       0.79      0.72      0.73        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2014\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "logreg_1 = LogisticRegression(class_weight='balanced')\n",
    "logreg_1.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "YEAR = 2015\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test_2 = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "logreg_2 = LogisticRegression(class_weight='balanced')\n",
    "logreg_2.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_2 = logreg_2.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test_2,y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from public_auc_veolia2 import score_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.array([y_pred,y_pred_2]).T\n",
    "true = np.array([Y_test,Y_test_2]).T\n",
    "\n",
    "print(score_function(pred,true))\n",
    "print('Votre score est de : 0.76149277963129')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.79      0.76        62\n",
      "          1       0.38      0.31      0.34        26\n",
      "\n",
      "avg / total       0.63      0.65      0.64        88\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.84      0.86        70\n",
      "          1       0.48      0.56      0.51        18\n",
      "\n",
      "avg / total       0.80      0.78      0.79        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2014\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "logreg_1 = SVC()\n",
    "logreg_1.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "YEAR = 2015\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test_2 = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "logreg_2 = SVC()\n",
    "logreg_2.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_2 = logreg_2.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test_2,y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605046197584\n",
      "Votre score est de : 0.76149277963129\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([y_pred,y_pred_2]).T\n",
    "true = np.array([Y_test,Y_test_2]).T\n",
    "\n",
    "print(score_function(pred,true))\n",
    "print('Votre score est de : 0.76149277963129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for the submission data\n",
    "sub_1 = logreg_1.predict(preprocess(input_submission,year=2015))\n",
    "sub_2 = logreg_2.predict(preprocess(input_submission,year=2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Submission formating\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = input_submission.index.tolist()\n",
    "submission['2014'] = sub_1[:]\n",
    "submission['2015'] = sub_2[:]\n",
    "submission = submission.set_index('Id')\n",
    "submission.to_csv('../submissions/with_augmentation_aftersplit.csv',index=True,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# With adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Solver liblinear does not support sample weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a330e1ae1a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrdm_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogreg_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrdm_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 sample_weight)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Early termination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \"\"\"\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         _check_solver_option(self.solver, self.multi_class, self.penalty,\n\u001b[0;32m-> 1148\u001b[0;31m                              self.dual, sample_weight)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_check_solver_option\u001b[0;34m(solver, multi_class, penalty, dual, sample_weight)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         raise ValueError(\"Solver %s does not support \"\n\u001b[0;32m--> 424\u001b[0;31m                          \"sample weights.\" % solver)\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Solver liblinear does not support sample weights."
     ]
    }
   ],
   "source": [
    "YEAR = 2014\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "rdm_1 = AdaBoostClassifier(base_estimator=,n_estimators=100)\n",
    "rdm_1.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = rdm_1.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.97      0.87        40\n",
      "          1       0.50      0.08      0.14        12\n",
      "\n",
      "avg / total       0.72      0.77      0.70        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2015\n",
    "\n",
    "X_train = input_train\n",
    "Y_train = preprocess_output(output_train, year = YEAR)\n",
    "X_test = input_test\n",
    "Y_test = preprocess_output(output_test, year = YEAR)\n",
    "\n",
    "rdm_2 = AdaBoostClassifier(n_estimators=100)\n",
    "rdm_2.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = rdm_1.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.array([y_pred,y_pred_2]).T\n",
    "true = np.array([Y_test,Y_test_2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59650000000000003"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_function(pred,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00     10344\n",
      "          1       0.98      1.00      0.99      4318\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14662\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99      9555\n",
      "          1       0.97      1.00      0.98      5107\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2015\n",
    "X = preprocess(input_train,year=YEAR)\n",
    "Y = preprocess_output(output_train,year=YEAR)\n",
    "\n",
    "X_train, X_test, y_train, y_test_1 = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "\n",
    "rdm_2015 = AdaBoostClassifier(n_estimators=1000)\n",
    "rdm_2015.fit(X_train, y_train)\n",
    "y_pred_1 = rdm_2015.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test_1,y_pred_1))\n",
    "\n",
    "\n",
    "YEAR = 2014\n",
    "X = preprocess(input_train,year=YEAR)\n",
    "Y = preprocess_output(output_train,year=YEAR)\n",
    "\n",
    "X_train, X_test, y_train, y_test_2 = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "rdm_2015.fit(X_train, y_train)\n",
    "\n",
    "y_pred_2 = rdm_2015.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test_2,y_pred_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79625925560382138"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.array([y_pred_1,y_pred_2])\n",
    "pred = pred.T\n",
    "true = np.array([y_test_2,y_test_2])\n",
    "true = true.T\n",
    "score_function(pred,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for the submission data\n",
    "sub_1 = rdm_2015.predict_proba(preprocess(input_test,year=2014))\n",
    "sub_2 = rdm_2015.predict_proba(preprocess(input_test,year=2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submission formating\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = input_test.index.tolist()\n",
    "submission['2014'] = sub_1[:,1]\n",
    "submission['2015'] = sub_2[:,1]\n",
    "submission = submission.set_index('Id')\n",
    "submission.to_csv('../submissions/data_augmentation_ada.csv',index=True,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
